from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
import torch
from torch.utils.data import TensorDataset, DataLoader
import torch.nn as nn
from torch.optim.lr_scheduler import ReduceLROnPlateau
import matplotlib.pyplot as plt
import random
import numpy as np
import pandas as pd
from pandas.api.types import is_numeric_dtype, is_bool_dtype

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def data_split(df, target_column, batch_size=32,
               test_size=0.15,       # ì „ì²´ ëŒ€ë¹„ test ë¹„ìœ¨
               val_size=0.1764706,    # ë‚¨ì€(1-test) ëŒ€ë¹„ val ë¹„ìœ¨ = 0.15/0.85
               random_state: int = 42,
               fixed_test_idx: np.ndarray | None = None
               ):
    # íƒ€ê²Ÿ ë¶„ë¦¬
    x = df.drop(columns=[target_column])
    y = df[target_column]
    print(f"x_list:",x.columns)
    # ë°ì´í„° ë¶„í•  train 70%, temp (test + valid) 30%
    if fixed_test_idx is not None:
        # ê³ ì • í…ŒìŠ¤íŠ¸ ì¸ë±ìŠ¤ ì‚¬ìš©

        N = len(df)
        test_idx = np.asarray(fixed_test_idx, dtype=int)
        if test_idx.ndim != 1 or test_idx.min() < 0 or test_idx.max() >= N:
            raise ValueError("fixed_test_idx out of range or invalid shape.")

        all_idx = np.arange(N)
        trainval_idx = np.setdiff1d(all_idx, test_idx)

        # ë‚¨ì€ 85%ì—ì„œ val_size(=0.17647) ë¹„ìœ¨ë¡œ Stratified ë¶„í•  â†’ ì „ì²´ 15%
        sss_val = StratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=random_state)
        (tr_rel, va_rel), = sss_val.split(x.iloc[trainval_idx], y.iloc[trainval_idx])

        tr_idx = trainval_idx[tr_rel]
        va_idx = trainval_idx[va_rel]
        te_idx = test_idx

        X_train, y_train = x.iloc[tr_idx], y.iloc[tr_idx]
        X_val,   y_val   = x.iloc[va_idx], y.iloc[va_idx]
        X_test,  y_test  = x.iloc[te_idx], y.iloc[te_idx]

    else:
        # ê¸°ì¡´ 70/15/15 ê²½ë¡œ(ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë˜ íŒŒë¼ë¯¸í„° ì‚¬ìš©)
        X_train, X_temp, y_train, y_temp = train_test_split(
            x, y, test_size=(test_size + val_size*(1-test_size)),   # = 0.30
            random_state=random_state, shuffle=True, stratify=y
        )
        X_val, X_test, y_val, y_test = train_test_split(
            X_temp, y_temp, test_size=0.5, random_state=random_state, shuffle=True, stratify=y_temp
        )

    scaler = StandardScaler()
    X_train_scd = scaler.fit_transform(X_train)
    X_val_scd = scaler.transform(X_val)
    X_test_scd = scaler.transform(X_test)

    # Tensor ë³€í™˜
    X_train_t = torch.tensor(X_train_scd, dtype=torch.float32)
    X_val_t = torch.tensor(X_val_scd, dtype=torch.float32)
    X_test_t = torch.tensor(X_test_scd, dtype=torch.float32)

    y_train_t = torch.tensor(y_train.values, dtype=torch.long)
    y_val_t = torch.tensor(y_val.values, dtype=torch.long)
    y_test_t = torch.tensor(y_test.values, dtype=torch.long)

    g = torch.Generator()
    g.manual_seed(random_state)
    
    train_loader = DataLoader(TensorDataset(X_train_t, y_train_t),
                              batch_size=batch_size, shuffle=True,
                              generator=g,
                              num_workers=0)
    val_loader = DataLoader(TensorDataset(X_val_t, y_val_t),
                            batch_size=batch_size, shuffle=False,
                            num_workers=0)
    test_loader = DataLoader(TensorDataset(X_test_t, y_test_t),
                             batch_size=batch_size, shuffle=False,
                            num_workers=0)

    return train_loader, val_loader, test_loader, X_train_t.shape[1]

def mlp_model(input_dim, output_dim=1, hidden_dims=None, dropouts=0.5, use_bn=True):
    """
    hidden_dims: List[int] (ì˜ˆ: [16, 32])  âŸ¶ Optunaì˜ h0, h1, ... ì— ëŒ€ì‘
    dropouts: float ë˜ëŠ” List[float]       âŸ¶ Optunaì˜ do0, do1, ... ì— ëŒ€ì‘
    """
    if hidden_dims is None or len(hidden_dims) == 0:
        hidden_dims = [64, 32]

    # dropouts ê¸¸ì´/íƒ€ì… ì •ê·œí™”
    if isinstance(dropouts, (int, float)):
        dropout_list = [float(dropouts)] * len(hidden_dims)
    else:
        assert len(dropouts) == len(hidden_dims), "dropouts ê¸¸ì´ê°€ hidden_dimsì™€ ë‹¬ë¼ìš”."
        dropout_list = [float(d) for d in dropouts]

    layers = []
    prev = input_dim
    for h, do in zip(hidden_dims, dropout_list):
        layers.append(nn.Linear(prev, h))
        if use_bn:
            layers.append(nn.BatchNorm1d(h, momentum=0.05))
        layers.append(nn.ReLU())
        layers.append(nn.Dropout(do))
        prev = h

    layers.append(nn.Linear(prev, output_dim))
    model = nn.Sequential(*layers)

    _init_he_hidden_xavier_out(model)  
    return model

def _init_he_hidden_xavier_out(model: nn.Module):
    linear_layers = [m for m in model.modules() if isinstance(m, nn.Linear)]
    if not linear_layers:
        return
    last_linear = linear_layers[-1]
    
    for m in model.modules():
        if isinstance(m, nn.Linear):
            if m is last_linear:
                nn.init.xavier_uniform_(m.weight, gain=1.0)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            else:
                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
                    
        elif isinstance(m, nn.BatchNorm1d):
            if m.weight is not None:
                nn.init.ones_(m.weight)
            if m.bias is not None:
                nn.init.zeros_(m.bias)
    
def train_model(model, criterion, optimizer, train_loader, val_loader, device, 
                num_epochs, patience, min_delta: float = 1e-4):
    train_losses = []
    val_losses = []

    best_val_loss = float('inf')
    patience_counter = 0
    best_model_state = None  # ì´ˆê¸°í™”

    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5,
                patience=2, threshold=1e-3, cooldown=1, min_lr=1e-5)
    
    for epoch in range(num_epochs + 1):
        model.train()
        running_loss = 0.0
        n_train = 0
        
        for inputs, labels in train_loader:
            inputs = inputs.float().to(device) 
            labels = labels.float().view(-1,1).to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * inputs.size(0)
            n_train += inputs.size(0)

        avg_train_loss = running_loss / max(1, n_train)
        train_losses.append(avg_train_loss)
        
        # Validation loss ê³„ì‚°
        model.eval()
        val_loss = 0.0
        n_val = 0
        with torch.no_grad():
            for val_inputs, val_labels in val_loader:
                val_inputs = val_inputs.float().to(device) 
                val_labels = val_labels.float().view(-1, 1).to(device)
                val_outputs = model(val_inputs)
                batch_loss = criterion(val_outputs, val_labels).item()
                val_loss += batch_loss * val_inputs.size(0)
                n_val += val_inputs.size(0)
                
        avg_val_loss = val_loss / max(1, n_val)
        val_losses.append(avg_val_loss)   
        
        if epoch == 0:
            ema = avg_val_loss
        else:
            beta = 0.6
            ema = beta * ema + (1- beta) * avg_val_loss
        
        monitor = ema
        
        scheduler.step(monitor)
        current_lr = optimizer.param_groups[0]['lr']

        print(f"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, "
        f"Val Loss: {avg_val_loss:.4f}, lr={current_lr:.2e}")

        # Early stopping ì²´í¬
        if monitor + min_delta < best_val_loss :  
            best_val_loss = monitor
            patience_counter = 0
            best_model_state = {k: v.detach().clone() for k, v in model.state_dict().items()}  
        else:
            patience_counter += 1
            print(f"  â†³ No improvement. Patience {patience_counter}/{patience}")
            if patience_counter >= patience:
                print("Early stopping triggered ğŸš¨")
                break

    # ìµœì ì˜ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¡œ ë³µì›
    """
    í•™ìŠµ ì¤‘ ê²€ì¦ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì•˜ë˜ ì‹œì ì˜ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ best_model_stateì— ì €ì¥
    ì´í›„ epochì—ì„œ ì„±ëŠ¥ì´ ë‚˜ë¹ ì ¸ë„ ê³„ì† í•™ìŠµë˜ë¯€ë¡œ, ì¢…ë£Œ ì‹œì ì˜ ëª¨ë¸ì€ bestê°€ ì•„ë‹ ìˆ˜ ìˆìŒ
    ë³µì› í•„ìš” 
    """
    if best_model_state is not None: 
        model.load_state_dict(best_model_state)

    return model, train_losses, val_losses

def plot_loss_curve(train_losses, val_losses):
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Loss Curve')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# ===== ê³¼ì í•© ì—¬ë¶€ í‰ê°€ ìœ í‹¸ =====
def evaluate_overfitting(train_losses, val_losses, window=5, gap_threshold=0.15):
    """
    [ADDED]
    ê°„ë‹¨í•œ ê³¼ì í•© íœ´ë¦¬ìŠ¤í‹±:
      1) ë§ˆì§€ë§‰ window í‰ê·  ê¸°ì¤€, Val - Train ìƒëŒ€ ê²©ì°¨ê°€ gap_threshold(%) ì´ìƒì´ë©´ ê³¼ì í•© ê²½ê³ 
      2) Val ì»¤ë¸Œê°€ í•˜ë½ ë©ˆì¶”ê³  ìƒìŠ¹ ì „í™˜(ë°”ë‹¥ ì°ê³  ìƒìŠ¹) íŒ¨í„´ì´ ìˆìœ¼ë©´ ê²½ê³ 
    """
    import numpy as np

    t = np.array(train_losses, dtype=float)
    v = np.array(val_losses, dtype=float)
    n = len(t)
    if n < max(3, window):
        print("[Overfit Check] ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ í‰ê°€ ìƒëµ")
        return {"overfit": False, "reason": "insufficient history"}

    # 1) ë§ˆì§€ë§‰ window êµ¬ê°„ í‰ê·  ê²©ì°¨
    best_idx = int(np.argmin(v))           # v: val_losses ë°°ì—´
    gap_at_best = (v[best_idx] - t[best_idx]) / (abs(t[best_idx]) + 1e-8)
    t_mean = t[-window:].mean()
    v_mean = v[-window:].mean()
    rel_gap_last = (v_mean - t_mean) / (abs(t_mean) + 1e-8)

    # 2) ìµœê·¼ 3í¬ì¸íŠ¸ë¡œ 'ìƒìŠ¹ ì „í™˜' ê°ì§€
    turn_up = False
    if n >= 3:
        turn_up = (v[-3] >= v[-2]) and (v[-1] > v[-2])
        
    rel_gap = min(gap_at_best, rel_gap_last)
    overfit = (rel_gap >= gap_threshold) or turn_up
    reason = []
    if rel_gap >= gap_threshold:
        reason.append(f"val-train ìƒëŒ€ ê²©ì°¨ {rel_gap*100:.1f}% â‰¥ {gap_threshold*100:.0f}%")
    if turn_up:
        reason.append("ê²€ì¦ ì†ì‹¤ì´ ìµœê·¼ ìƒìŠ¹ ì „í™˜ íŒ¨í„´")

    msg = "ê³¼ì í•© ì§•í›„ ê°ì§€" if overfit else "ê³¼ì í•© ëšœë ·í•˜ì§€ ì•ŠìŒ"
    print(f"[Overfit Check] {msg} | last{window}_mean: train={t_mean:.4f}, val={v_mean:.4f}, gap={rel_gap*100:.1f}%")
    if reason:
        print("  - ê·¼ê±°:", "; ".join(reason))

    return {"overfit": bool(overfit), "gap": float(rel_gap), "turn_up": bool(turn_up), "reason": "; ".join(reason)}
