from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
import torch
from torch.utils.data import TensorDataset, DataLoader
import torch.nn as nn
import matplotlib.pyplot as plt
import random
import numpy as np
import pandas as pd
from pandas.api.types import is_numeric_dtype, is_bool_dtype

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)

    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def data_split(df, target_column, batch_size=32,
               test_size=0.15,       # ì „ì²´ ëŒ€ë¹„ test ë¹„ìœ¨
               val_size=0.1764706,    # ë‚¨ì€(1-test) ëŒ€ë¹„ val ë¹„ìœ¨ = 0.15/0.85
               random_state: int = 42,
               fixed_test_idx: np.ndarray | None = None
               ):
    # íƒ€ê²Ÿ ë¶„ë¦¬
    x = df.drop(columns=[target_column])
    y = df[target_column]

    # ë°ì´í„° ë¶„í•  train 70%, temp (test + valid) 30%
    if fixed_test_idx is not None:
        # ê³ ì • í…ŒìŠ¤íŠ¸ ì¸ë±ìŠ¤ ì‚¬ìš©

        N = len(df)
        test_idx = np.asarray(fixed_test_idx, dtype=int)
        if test_idx.ndim != 1 or test_idx.min() < 0 or test_idx.max() >= N:
            raise ValueError("fixed_test_idx out of range or invalid shape.")

        all_idx = np.arange(N)
        trainval_idx = np.setdiff1d(all_idx, test_idx)

        # ë‚¨ì€ 85%ì—ì„œ val_size(=0.17647) ë¹„ìœ¨ë¡œ Stratified ë¶„í•  â†’ ì „ì²´ 15%
        sss_val = StratifiedShuffleSplit(n_splits=1, test_size=val_size, random_state=random_state)
        (tr_rel, va_rel), = sss_val.split(x.iloc[trainval_idx], y.iloc[trainval_idx])

        tr_idx = trainval_idx[tr_rel]
        va_idx = trainval_idx[va_rel]
        te_idx = test_idx

        X_train, y_train = x.iloc[tr_idx], y.iloc[tr_idx]
        X_val,   y_val   = x.iloc[va_idx], y.iloc[va_idx]
        X_test,  y_test  = x.iloc[te_idx], y.iloc[te_idx]

    else:
        # ê¸°ì¡´ 70/15/15 ê²½ë¡œ(ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ë˜ íŒŒë¼ë¯¸í„° ì‚¬ìš©)
        X_train, X_temp, y_train, y_temp = train_test_split(
            x, y, test_size=(test_size + val_size*(1-test_size)),   # = 0.30
            random_state=random_state, shuffle=True, stratify=y
        )
        X_val, X_test, y_val, y_test = train_test_split(
            X_temp, y_temp, test_size=0.5, random_state=random_state, shuffle=True, stratify=y_temp
        )

    scaler = StandardScaler()
    X_train_scd = scaler.fit_transform(X_train)
    X_val_scd = scaler.transform(X_val)
    X_test_scd = scaler.transform(X_test)

    # Tensor ë³€í™˜
    X_train_t = torch.tensor(X_train_scd, dtype=torch.float32)
    X_val_t = torch.tensor(X_val_scd, dtype=torch.float32)
    X_test_t = torch.tensor(X_test_scd, dtype=torch.float32)

    y_train_t = torch.tensor(y_train.values, dtype=torch.long)
    y_val_t = torch.tensor(y_val.values, dtype=torch.long)
    y_test_t = torch.tensor(y_test.values, dtype=torch.long)

    train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(TensorDataset(X_val_t, y_val_t), batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(TensorDataset(X_test_t, y_test_t), batch_size=batch_size, shuffle=False)

    return train_loader, val_loader, test_loader, X_train_t.shape[1]

def data_split_by_index(
    df: pd.DataFrame,
    target_column: str,
    train_idx: np.ndarray,
    val_idx: np.ndarray,
    test_idx: np.ndarray,
    batch_size: int = 32,
):
    """
    K-Fold/ì»¤ìŠ¤í…€ ë¶„í• ìš©.
    - ìŠ¤ì¼€ì¼ëŸ¬ëŠ” train foldì—ë§Œ fit â†’ val/testì— transform (ëˆ„ìˆ˜ ë°©ì§€)
    - ìˆ«ìí˜•ë§Œ ì‚¬ìš©, target ì œì™¸
    - torch Dataset/DataLoader ë°˜í™˜ ìŠ¤í™ì€ ê¸°ì¡´ data_splitê³¼ ë™ì¼
    ë°˜í™˜: (train_loader, val_loader, test_loader, input_dim)
    """

    # ìˆ«ìí˜•ë§Œ ì‚¬ìš©
    feats = [c for c in df.columns 
             if c != target_column and (is_numeric_dtype(df[c])or is_bool_dtype(df[c]))
             ]
    if len(feats) == 0:
        raise ValueError("No numeric features after filtering."
                         f"(cols={list(df.columns)}, target={target_column})"
                         )
    
    X_df = df[feats].copy()

    for c in X_df.columns:
        if is_bool_dtype(X_df[c]) or str(X_df[c].dtype).lower() in ("boolean", "booleandtype"):
            X_df[c] = X_df[c].astype("float32").fillna(0.0)
            
    
    X = X_df.astype("float32").to_numpy(dtype=np.float32)
    y = df[target_column].values.astype(np.int64)

    X_tr, y_tr = X[train_idx], y[train_idx]
    X_va, y_va = X[val_idx], y[val_idx]
    X_te, y_te = X[test_idx], y[test_idx]

    # ìŠ¤ì¼€ì¼ë§: trainì— fit â†’ ë‚˜ë¨¸ì§€ì— ì ìš©
    scaler = StandardScaler()
    X_tr = scaler.fit_transform(X_tr)
    X_va = scaler.transform(X_va)
    X_te = scaler.transform(X_te)

    # í…ì„œ/ë¡œë”
    X_tr_t, y_tr_t = torch.tensor(X_tr), torch.tensor(y_tr)
    X_va_t, y_va_t = torch.tensor(X_va), torch.tensor(y_va)
    X_te_t, y_te_t = torch.tensor(X_te), torch.tensor(y_te)

    train_loader = DataLoader(TensorDataset(X_tr_t, y_tr_t), batch_size=batch_size, shuffle=True)
    val_loader   = DataLoader(TensorDataset(X_va_t, y_va_t), batch_size=batch_size, shuffle=False)
    test_loader  = DataLoader(TensorDataset(X_te_t, y_te_t), batch_size=batch_size, shuffle=False)

    input_dim = X_tr.shape[1]
    
    return train_loader, val_loader, test_loader, input_dim

def mlp_model(input_dim, output_dim=2):
    # [CHANGED]: BN ì œê±°, Dropout ì™„í™”(0.3â†’0.10, 0.2â†’0.05) â€” ê¸°ì¤€ëª¨ë¸ ì•ˆì •ì„±/í•´ì„ì„± ê°•í™”
    model = nn.Sequential(
        nn.Linear(input_dim, 64),
        nn.ReLU(),
        nn.Dropout(0.10),     

        nn.Linear(64, 32),
        nn.ReLU(),
        nn.Dropout(0.05),     

        nn.Linear(32, output_dim)
    )
    return model
# input: X_train_t.shape[1]

def train_model(model, criterion, optimizer, train_loader, val_loader, device, patience, num_epochs, scheduler=None):
    train_losses = []
    val_losses = []

    best_val_loss = float('inf')
    patience_counter = 0
    best_model_state = None  # ì´ˆê¸°í™”

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        for inputs, labels in train_loader:
            inputs, labels = inputs.float().to(device), labels.long().to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        avg_train_loss = running_loss / len(train_loader)
        train_losses.append(avg_train_loss)

        # Validation loss ê³„ì‚°
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for val_inputs, val_labels in val_loader:
                val_inputs, val_labels = val_inputs.float().to(device), val_labels.long().to(device)
                val_outputs = model(val_inputs)
                val_loss += criterion(val_outputs, val_labels).item()
        avg_val_loss = val_loss / len(val_loader)
        val_losses.append(avg_val_loss)
        
        # if scheduler is not None:   # [ADDED]
        #     scheduler.step(avg_val_loss)

        print(f"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}")

        # Early stopping ì²´í¬
        if avg_val_loss < best_val_loss - 1e-9:  
            best_val_loss = avg_val_loss
            patience_counter = 0
            best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}  # [ADDED] ì•ˆì „ ì €ì¥
        else:
            patience_counter += 1
            print(f"  â†³ No improvement. Patience {patience_counter}/{patience}")
            if patience_counter >= patience:
                print("Early stopping triggered ğŸš¨")
                break

    # ìµœì ì˜ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¡œ ë³µì›
    """
    í•™ìŠµ ì¤‘ ê²€ì¦ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ì•˜ë˜ ì‹œì ì˜ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ best_model_stateì— ì €ì¥
    ì´í›„ epochì—ì„œ ì„±ëŠ¥ì´ ë‚˜ë¹ ì ¸ë„ ê³„ì† í•™ìŠµë˜ë¯€ë¡œ, ì¢…ë£Œ ì‹œì ì˜ ëª¨ë¸ì€ bestê°€ ì•„ë‹ ìˆ˜ ìˆìŒ
    ë³µì› í•„ìš” 
    """
    if best_model_state is not None:  # [ADDED]
        model.load_state_dict(best_model_state)

    return model, train_losses, val_losses

def plot_loss_curve(train_losses, val_losses):
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='Train Loss')
    plt.plot(val_losses, label='Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Loss Curve')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# ===== ê³¼ì í•© ì—¬ë¶€ í‰ê°€ ìœ í‹¸ =====
def evaluate_overfitting(train_losses, val_losses, window=5, gap_threshold=0.15):
    """
    [ADDED]
    ê°„ë‹¨í•œ ê³¼ì í•© íœ´ë¦¬ìŠ¤í‹±:
      1) ë§ˆì§€ë§‰ window í‰ê·  ê¸°ì¤€, Val - Train ìƒëŒ€ ê²©ì°¨ê°€ gap_threshold(%) ì´ìƒì´ë©´ ê³¼ì í•© ê²½ê³ 
      2) Val ì»¤ë¸Œê°€ í•˜ë½ ë©ˆì¶”ê³  ìƒìŠ¹ ì „í™˜(ë°”ë‹¥ ì°ê³  ìƒìŠ¹) íŒ¨í„´ì´ ìˆìœ¼ë©´ ê²½ê³ 
    """
    import numpy as np

    t = np.array(train_losses, dtype=float)
    v = np.array(val_losses, dtype=float)
    n = len(t)
    if n < max(3, window):
        print("[Overfit Check] ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ í‰ê°€ ìƒëµ")
        return {"overfit": False, "reason": "insufficient history"}

    # 1) ë§ˆì§€ë§‰ window êµ¬ê°„ í‰ê·  ê²©ì°¨
    t_mean = t[-window:].mean()
    v_mean = v[-window:].mean()
    rel_gap = (v_mean - t_mean) / (abs(t_mean) + 1e-8)

    # 2) ìµœê·¼ 3í¬ì¸íŠ¸ë¡œ 'ìƒìŠ¹ ì „í™˜' ê°ì§€
    turn_up = False
    if n >= 3:
        turn_up = (v[-3] >= v[-2]) and (v[-1] > v[-2])

    overfit = (rel_gap >= gap_threshold) or turn_up
    reason = []
    if rel_gap >= gap_threshold:
        reason.append(f"val-train ìƒëŒ€ ê²©ì°¨ {rel_gap*100:.1f}% â‰¥ {gap_threshold*100:.0f}%")
    if turn_up:
        reason.append("ê²€ì¦ ì†ì‹¤ì´ ìµœê·¼ ìƒìŠ¹ ì „í™˜ íŒ¨í„´")

    msg = "ê³¼ì í•© ì§•í›„ ê°ì§€" if overfit else "ê³¼ì í•© ëšœë ·í•˜ì§€ ì•ŠìŒ"
    print(f"[Overfit Check] {msg} | last{window}_mean: train={t_mean:.4f}, val={v_mean:.4f}, gap={rel_gap*100:.1f}%")
    if reason:
        print("  - ê·¼ê±°:", "; ".join(reason))

    return {"overfit": bool(overfit), "gap": float(rel_gap), "turn_up": bool(turn_up), "reason": "; ".join(reason)}
